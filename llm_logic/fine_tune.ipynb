{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vavvtf15ALnM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    PreTrainedTokenizerBase\n",
        ")\n",
        "from typing import List, Dict, Union\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Config ------------------ #\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "data_path = os.path.join(os.getcwd(), \"dataset\", \"medical_meadow_wikidoc\", \"fine_tune_data.jsonl\")\n",
        "output_dir = \"./neo_outputs\"\n",
        "logging_dir = \"./neo_logs\"\n"
      ],
      "metadata": {
        "id": "VpGvuckJAUKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Load Model & Tokenizer ------------------ #\n",
        "try:\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    tokenizer.padding_side = \"right\"\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load model/tokenizer: {e}\")\n"
      ],
      "metadata": {
        "id": "2IL1osUqAUMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Load Dataset ------------------ #\n",
        "try:\n",
        "    abs_path = os.path.abspath(data_path)\n",
        "    print(f\"Loading dataset from: {abs_path}\")\n",
        "\n",
        "    if not os.path.exists(abs_path) or os.path.getsize(abs_path) == 0:\n",
        "        raise FileNotFoundError(\"Dataset file does not exist or is empty.\")\n",
        "\n",
        "    with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if not lines:\n",
        "        raise ValueError(\"Dataset file is empty.\")\n",
        "\n",
        "    records = [json.loads(line) for line in lines]\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Parsed DataFrame is empty.\")\n",
        "\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load dataset: {e}\")\n"
      ],
      "metadata": {
        "id": "MkcKgEXqAUOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Tokenization ------------------ #\n",
        "def tokenize_function(example):\n",
        "    try:\n",
        "        text = f\"### Question:\\n{example['prompt']}\\n\\n### Answer:\\n{example['response']}\"\n",
        "        tokens = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"],\n",
        "            \"attention_mask\": tokens[\"attention_mask\"],\n",
        "            \"labels\": tokens[\"input_ids\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Tokenization failed for example: {example} â€” {e}\")\n",
        "        return {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "        }\n",
        "\n",
        "try:\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = dataset.map(tokenize_function, remove_columns=[\"prompt\", \"response\"])\n",
        "    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Tokenization or filtering failed: {e}\")\n"
      ],
      "metadata": {
        "id": "IHnwIztTAUQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Train / Eval Split ------------------ #\n",
        "try:\n",
        "    split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = split[\"train\"]\n",
        "    eval_dataset = split[\"test\"]\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Dataset splitting failed: {e}\")\n"
      ],
      "metadata": {
        "id": "RYaP7MjrAUTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ Custom Data Collator ------------------ #\n",
        "class CausalDataCollator:\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        features = [f for f in features if len(f[\"input_ids\"]) > 0]\n",
        "        if len(features) == 0:\n",
        "            raise ValueError(\"No valid sequences in batch.\")\n",
        "\n",
        "        input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
        "        attention_mask = [torch.tensor(f[\"attention_mask\"]) for f in features]\n",
        "        labels = [torch.tensor(f[\"labels\"]) for f in features]\n",
        "\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "data_collator = CausalDataCollator(tokenizer)\n"
      ],
      "metadata": {
        "id": "UmQbvNkdAUVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Training Arguments ------------------ #\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=logging_dir,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "MI4XAeGpAUXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Trainer Setup ------------------ #\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "2G4nHFGmAUZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Sample Batch Check ------------------ #\n",
        "print(\"Checking a sample batch from the data collator...\")\n",
        "try:\n",
        "    for i, batch in enumerate(trainer.get_train_dataloader()):\n",
        "        if i == 0:\n",
        "            for k, v in batch.items():\n",
        "                print(f\"{k}: shape={v.shape}, dtype={v.dtype}\")\n",
        "            break\n",
        "except Exception as e:\n",
        "    print(f\"Error during batch inspection: {e}\")\n"
      ],
      "metadata": {
        "id": "iqGm7hCzAUb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Training ------------------ #\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Training failed: {e}\")\n"
      ],
      "metadata": {
        "id": "VaCCU_igAUd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Save Model ------------------ #\n",
        "try:\n",
        "    final_output_path = os.path.join(output_dir, \"final\")\n",
        "    print(f\"Saving model to: {final_output_path}\")\n",
        "    trainer.save_model(final_output_path)\n",
        "    tokenizer.save_pretrained(final_output_path)\n",
        "    print(\"Training complete.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Saving model failed: {e}\")\n"
      ],
      "metadata": {
        "id": "dDXK-uM8AUgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EVYTmKVIAUib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}